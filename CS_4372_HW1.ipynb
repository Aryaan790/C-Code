{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryaan790/C-Code/blob/main/CS_4372_HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAU603Pi16Oh"
      },
      "outputs": [],
      "source": [
        "# Import the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from statsmodels.sandbox.regression.predstd import wls_prediction_std"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "df = pd.read_csv(\"/content/winequality-red.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ONZcwWcD2Bc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info"
      ],
      "metadata": {
        "id": "X4RuoSZRj9jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for null values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "WdiFv1b3Fs7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "hqDS_V0NLcjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot various graphs to find quality of wine\n",
        "df.plot(x='alcohol',y='quality',style='.',color='r')\n",
        "plt.title('alcohol vs quality')\n",
        "plt.xlabel('alcohol')\n",
        "plt.ylabel('quality')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UMR8xhGiIcop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.plot(x='free sulfur dioxide',y='total sulfur dioxide', style='.',color='g')\n",
        "plt.title('free sulfur dioxide vs total sulfur dioxide')\n",
        "plt.xlabel('free sulfur dioxide')\n",
        "plt.ylabel('total sulfur dioxide')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4GR9_T3zI68W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data is easily scattered on features\n",
        "df.hist(bins=20, figsize=(10, 10))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mdXtYMa4JXd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking which value of citric acid can make changes in quality\n",
        "sns.catplot(data=df, kind=\"bar\",x=\"quality\",y=\"citric acid\",palette=\"pastel\",alpha=.5, height=5)"
      ],
      "metadata": {
        "id": "PUkMOFwfJeg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=\"quality\",y=\"citric acid\",data=df)"
      ],
      "metadata": {
        "id": "9U-TJPGdJqC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=\"quality\",y=\"fixed acidity\",data=df)"
      ],
      "metadata": {
        "id": "Iw9YmzXLKNxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Observe correlation matrix\n",
        "correlation_matrix = df.corr().round(2)\n",
        "sns.heatmap(data=correlation_matrix, annot=True)"
      ],
      "metadata": {
        "id": "lvP2oMS6LXb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define X and Y\n",
        "df['winequality']=[1 if x>=6 else 0 for x in df['quality']]\n",
        "X=df.drop(['quality','winequality'],axis = 1)\n",
        "y=df['winequality']\n",
        "n = y.shape[0]\n",
        "\n",
        "# separate the data into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "\n",
        "# Scale the data\n",
        "mm = MinMaxScaler()\n",
        "fit=mm.fit(X_train)\n",
        "X_train=fit.transform(X_train)\n",
        "X_test=fit.transform(X_test)"
      ],
      "metadata": {
        "id": "TGU2J--1Evmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SGDRegressor\n",
        "# train the model using the training set\n",
        "max_iter = (int)(np.ceil(10**6 / n))\n",
        "reg_model = SGDRegressor(max_iter=max_iter)\n",
        "reg_model.fit(X_train, y_train)\n",
        "\n",
        "# predict the target variable using the test set and the trained model\n",
        "y_pred_train_sgd = reg_model.predict(X_train)\n",
        "y_pred_test_sgd = reg_model.predict(X_test)"
      ],
      "metadata": {
        "id": "WIeUMsKWYE60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid search - this will take about 1 minute.\n",
        "param_grid = {\n",
        "    'alpha': 10.0 ** -np.arange(1, 7),\n",
        "    'loss': ['squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
        "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'learning_rate': ['constant', 'optimal', 'invscaling'],\n",
        "    'tol':[1e-10, 1e-3],\n",
        "    'eta0':[0.001, 0.01]\n",
        "}\n",
        "\n",
        "# Hyperparameter Tuning\n",
        "clf = GridSearchCV(reg_model, param_grid, n_jobs = 8, verbose = 3)\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "bOGFb7ghcSDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bElAxgM5AgVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best Score: ', clf.best_score_)\n",
        "print('Best Parameters: ', clf.best_params_)\n",
        "print(\"Best Estimator: \",  clf.best_estimator_)\n",
        "#print(\"Scores for alphas: \", clf.grid_scores_)"
      ],
      "metadata": {
        "id": "DcX0NmawsAk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model evaluation for training set\n",
        "rmse = (np.sqrt(mean_squared_error(y_train, y_pred_train_sgd)))\n",
        "r2 = r2_score(y_train, y_pred_train_sgd)\n",
        "\n",
        "print(\"The model performance for training set\")\n",
        "print(\"--------------------------------------\")\n",
        "print('RMSE is {}'.format(rmse))\n",
        "print('R2 score is {}'.format(r2))\n",
        "print(\"\\n\")\n",
        "\n",
        "# model evaluation for testing set\n",
        "rmse = (np.sqrt(mean_squared_error(y_test, y_pred_test_sgd)))\n",
        "r2 = r2_score(y_test, y_pred_test_sgd)\n",
        "\n",
        "print(\"The model performance for testing set\")\n",
        "print(\"--------------------------------------\")\n",
        "print('RMSE is {}'.format(rmse))\n",
        "print('R2 score is {}'.format(r2))"
      ],
      "metadata": {
        "id": "Zu7WIKThc52r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OLS\n",
        "# train the model using the training set\n",
        "reg_model = LinearRegression()\n",
        "reg_model.fit(X_train, y_train)\n",
        "\n",
        "# predict the target variable using the test set and the trained model\n",
        "y_pred_train = reg_model.predict(X_train)\n",
        "y_pred_test = reg_model.predict(X_test)"
      ],
      "metadata": {
        "id": "7rBsNPBjgGPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model evaluation for training set\n",
        "rmse = (np.sqrt(mean_squared_error(y_train, y_pred_train)))\n",
        "r2 = r2_score(y_train, y_pred_train)\n",
        "\n",
        "print(\"The model performance for training set\")\n",
        "print(\"--------------------------------------\")\n",
        "print('RMSE is {}'.format(rmse))\n",
        "print('R2 score is {}'.format(r2))\n",
        "print(\"\\n\")\n",
        "\n",
        "# model evaluation for testing set\n",
        "rmse = (np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
        "r2 = r2_score(y_test, y_pred_test)\n",
        "\n",
        "print(\"The model performance for testing set\")\n",
        "print(\"--------------------------------------\")\n",
        "print('RMSE is {}'.format(rmse))\n",
        "print('R2 score is {}'.format(r2))"
      ],
      "metadata": {
        "id": "MUYLhKrtqHlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ordinary Least Square (OLS) Principle\n",
        "x = sm.add_constant(X_train)\n",
        "model = sm.OLS(y_train, x).fit()\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "J2iNL_7rgwQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ypred = model.predict(x)\n",
        "rmse = sm.tools.eval_measures.rmse(y_train, ypred)\n",
        "print(rmse)"
      ],
      "metadata": {
        "id": "ulcQ6ZoSqEeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot results from linear model and SGD\n",
        "plt.scatter(y_test, y_pred, label='LR')\n",
        "plt.scatter(y_test, y_pred_test_sgd, label='SGD')\n",
        "plt.xlabel(\"Prediction: $\\hat{Y}_i$\")\n",
        "plt.ylabel(\"Actual: $Y_i$\")\n",
        "plt.title(\"Actual vs Predicted\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.) # Place a legend to the right of this smaller subplot.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GvxSVIXXMry3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}